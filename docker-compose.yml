version: '3.8'

services:
  redis:
    image: redis:7-alpine
    container_name: studio-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    command: redis-server --save 60 1 --loglevel warning
  # Optional: Local AI container for NVIDIA rigs
  # Uncomment if you want to run Ollama inside Docker with GPU support
  # local-ai:
  #   image: ollama/ollama
  #   container_name: studio-ollama
  #   ports:
  #     - "11434:11434"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

volumes:
  redis_data:
